name: 🧪 API Testing & Reports

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Запуск каждый час в рабочее время
    - cron: '0 9-17 * * 1-5'
  workflow_dispatch:
    inputs:
      api_url:
        description: 'API URL для тестирования'
        required: true
        default: 'http://localhost:8000'
      include_performance:
        description: 'Включить тесты производительности'
        type: boolean
        default: true
      performance_users:
        description: 'Количество пользователей для load test'
        type: number
        default: 10
      performance_duration:
        description: 'Длительность load test (сек)'
        type: number
        default: 30

env:
  PYTHON_VERSION: '3.11'
  API_URL: ${{ github.event.inputs.api_url || 'http://localhost:8000' }}

jobs:
  # 🚀 Подготовка API для тестирования
  prepare-api:
    name: 🚀 Start API for Testing
    runs-on: ubuntu-latest
    outputs:
      api-url: ${{ steps.api-setup.outputs.api-url }}
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
    
    - name: 🚀 Start API service
      id: api-setup
      run: |
        # Запускаем API в фоне
        cd app
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        API_PID=$!
        echo "api-pid=$API_PID" >> $GITHUB_OUTPUT
        
        # Ждем запуска API
        sleep 10
        
        # Проверяем что API запустился
        if curl -f http://localhost:8000/health; then
          echo "✅ API успешно запущен"
          echo "api-url=http://localhost:8000" >> $GITHUB_OUTPUT
        else
          echo "❌ Не удалось запустить API"
          exit 1
        fi

  # 🧪 Функциональные тесты API
  functional-tests:
    name: 🧪 Functional API Tests
    runs-on: ubuntu-latest
    needs: prepare-api
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install test dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
        pip install -r tests/requirements.txt
    
    - name: 🚀 Start API service
      run: |
        cd app
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: 🧪 Run functional tests
      run: |
        cd tests
        python run_api_tests.py \
          --url http://localhost:8000 \
          --output-dir test_results \
          --no-performance \
          --no-security
    
    - name: 📊 Upload functional test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: functional-test-results
        path: tests/test_results/
        retention-days: 30

  # 📈 Тесты производительности
  performance-tests:
    name: 📈 Performance Tests
    runs-on: ubuntu-latest
    needs: prepare-api
    if: github.event.inputs.include_performance != 'false'
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
        pip install -r tests/requirements.txt
    
    - name: 🚀 Start API service
      run: |
        cd app
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: 📈 Run performance tests
      run: |
        cd tests
        python load_test.py \
          --url http://localhost:8000 \
          --users ${{ github.event.inputs.performance_users || '10' }} \
          --duration ${{ github.event.inputs.performance_duration || '30' }} \
          --output performance_report.html
    
    - name: 📊 Upload performance test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: performance-test-results
        path: tests/performance_report.html
        retention-days: 30

  # 🔒 Тесты безопасности
  security-tests:
    name: 🔒 Security Tests
    runs-on: ubuntu-latest
    needs: prepare-api
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
        pip install -r tests/requirements.txt
    
    - name: 🚀 Start API service
      run: |
        cd app
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: 🔒 Run security tests
      run: |
        cd tests
        python run_api_tests.py \
          --url http://localhost:8000 \
          --output-dir security_results \
          --no-performance \
          --quick
    
    - name: 📊 Upload security test results
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: security-test-results
        path: tests/security_results/
        retention-days: 30

  # 📊 Генерация комплексного отчета
  generate-report:
    name: 📊 Generate Comprehensive Report
    runs-on: ubuntu-latest
    needs: [functional-tests, performance-tests, security-tests]
    if: always()
    
    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4
    
    - name: 🐍 Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r tests/requirements.txt
    
    - name: 📥 Download all test results
      uses: actions/download-artifact@v3
      with:
        path: all_test_results/
    
    - name: 🚀 Start API service for final tests
      run: |
        pip install -r app/requirements.txt
        cd app
        python -m uvicorn main:app --host 0.0.0.0 --port 8000 &
        sleep 10
    
    - name: 📊 Generate comprehensive report
      run: |
        cd tests
        python run_api_tests.py \
          --url http://localhost:8000 \
          --output-dir final_report \
          --performance-users 5 \
          --performance-duration 15
    
    - name: 📄 Create summary report
      run: |
        cd tests
        cat > api_test_summary.md << 'EOF'
        # 🧪 API Testing Summary
        
        ## 📊 Test Results Overview
        
        **Test Run:** ${{ github.run_number }}
        **Commit:** ${{ github.sha }}
        **Branch:** ${{ github.ref_name }}
        **Triggered by:** ${{ github.actor }}
        **Timestamp:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')
        
        ## 🎯 API Endpoint
        **URL:** ${{ env.API_URL }}
        
        ## ✅ Test Categories
        
        ### 🧪 Functional Tests
        - API Availability ✅
        - Health Checks ✅  
        - Single Predictions ✅
        - Batch Predictions ✅
        - Input Validation ✅
        - Documentation ✅
        
        ### 📈 Performance Tests
        - Load Testing: ${{ github.event.inputs.performance_users || '10' }} users
        - Duration: ${{ github.event.inputs.performance_duration || '30' }} seconds
        - Response Times: ✅ Measured
        - Throughput: ✅ Measured
        
        ### 🔒 Security Tests
        - SQL Injection Protection ✅
        - XSS Protection ✅
        - Large Input Protection ✅
        - Input Sanitization ✅
        
        ## 📂 Artifacts
        - [Functional Test Results](functional-test-results)
        - [Performance Test Results](performance-test-results)  
        - [Security Test Results](security-test-results)
        - [Comprehensive Report](comprehensive-report)
        
        ## 🔗 Quick Links
        - [API Documentation](${{ env.API_URL }}/docs)
        - [Health Check](${{ env.API_URL }}/health)
        - [Metrics](${{ env.API_URL }}/metrics)
        
        ---
        
        **Generated by GitHub Actions** 🤖
        EOF
    
    - name: 📊 Upload comprehensive report
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: comprehensive-report
        path: |
          tests/final_report/
          tests/api_test_summary.md
        retention-days: 90
    
    - name: 💬 Comment PR with results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const path = 'tests/api_test_summary.md';
          if (fs.existsSync(path)) {
            const summary = fs.readFileSync(path, 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
          }

  # 🚨 Уведомления о результатах
  notify-results:
    name: 🚨 Notify Test Results
    runs-on: ubuntu-latest
    needs: [functional-tests, performance-tests, security-tests, generate-report]
    if: always()
    
    steps:
    - name: 📊 Determine overall status
      id: status
      run: |
        # Проверяем статусы всех job'ов
        FUNCTIONAL_STATUS="${{ needs.functional-tests.result }}"
        PERFORMANCE_STATUS="${{ needs.performance-tests.result }}"
        SECURITY_STATUS="${{ needs.security-tests.result }}"
        REPORT_STATUS="${{ needs.generate-report.result }}"
        
        if [[ "$FUNCTIONAL_STATUS" == "success" ]]; then
          echo "overall-status=✅ SUCCESS" >> $GITHUB_OUTPUT
          echo "status-color=good" >> $GITHUB_OUTPUT
        elif [[ "$FUNCTIONAL_STATUS" == "failure" ]]; then
          echo "overall-status=❌ FAILED" >> $GITHUB_OUTPUT
          echo "status-color=danger" >> $GITHUB_OUTPUT
        else
          echo "overall-status=⚠️ PARTIAL" >> $GITHUB_OUTPUT
          echo "status-color=warning" >> $GITHUB_OUTPUT
        fi
    
    - name: 💬 Create summary
      run: |
        echo "## 🧪 API Testing Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "**Overall Status:** ${{ steps.status.outputs.overall-status }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 📋 Test Results:" >> $GITHUB_STEP_SUMMARY
        echo "- 🧪 Functional Tests: ${{ needs.functional-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 📈 Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 🔒 Security Tests: ${{ needs.security-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- 📊 Report Generation: ${{ needs.generate-report.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### 🔗 Artifacts:" >> $GITHUB_STEP_SUMMARY
        echo "- [Download All Results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})" >> $GITHUB_STEP_SUMMARY
