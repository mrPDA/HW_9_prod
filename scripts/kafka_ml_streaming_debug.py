#!/usr/bin/env python3
"""
Kafka ML Streaming Job - Debug –≤–µ—Ä—Å–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –æ—à–∏–±–æ–∫
"""
import argparse
import sys
import time
import logging
from datetime import datetime
from pyspark.sql import SparkSession

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ DEBUG –≤–µ—Ä—Å–∏–∏ Kafka ML Streaming Job...")
    
    try:
        # 1. –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        parser = argparse.ArgumentParser(description='Kafka ML Streaming Job - Debug')
        parser.add_argument('--kafka-brokers', required=True)
        parser.add_argument('--input-topic', required=True)
        parser.add_argument('--output-topic', required=True)
        parser.add_argument('--metrics-topic', required=True)
        parser.add_argument('--mlflow-uri', required=True)
        parser.add_argument('--model-name', required=True)
        parser.add_argument('--model-stage', default='champion')
        parser.add_argument('--fraud-threshold', type=float, default=0.5)
        parser.add_argument('--batch-timeout', default='30 seconds')
        parser.add_argument('--max-offsets', type=int, default=1000)
        parser.add_argument('--demo-duration', type=int, default=60, help='Demo duration in seconds')
        parser.add_argument('--checkpoint-location', help='Checkpoint location (ignored in demo)')
        parser.add_argument('--app-name', help='Application name (ignored in demo)')
        
        args = parser.parse_args()
        logger.info("‚úÖ –ê—Ä–≥—É–º–µ–Ω—Ç—ã —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã")
        
        # 2. –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        logger.info("üìã –ü–û–õ–£–ß–ï–ù–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
        logger.info(f"  üîó Kafka Brokers: '{args.kafka_brokers}'")
        logger.info(f"  üì• Input Topic: '{args.input_topic}'")
        logger.info(f"  üì§ Output Topic: '{args.output_topic}'")
        logger.info(f"  üìä Metrics Topic: '{args.metrics_topic}'")
        logger.info(f"  ü§ñ MLflow URI: '{args.mlflow_uri}'")
        logger.info(f"  üèÜ Model Name: '{args.model_name}'")
        logger.info(f"  üéØ Model Stage: '{args.model_stage}'")
        logger.info(f"  ‚ö° Fraud Threshold: {args.fraud_threshold}")
        logger.info(f"  ‚è±Ô∏è Demo Duration: {args.demo_duration} —Å–µ–∫—É–Ω–¥")
        
        # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        required_params = {
            'kafka_brokers': args.kafka_brokers,
            'input_topic': args.input_topic,
            'output_topic': args.output_topic,
            'metrics_topic': args.metrics_topic,
            'mlflow_uri': args.mlflow_uri,
            'model_name': args.model_name
        }
        
        empty_params = [k for k, v in required_params.items() if not v or v == 'None']
        if empty_params:
            logger.error(f"‚ùå –û–®–ò–ë–ö–ê: –ü—É—Å—Ç—ã–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: {empty_params}")
            sys.exit(1)
        
        logger.info("‚úÖ –í—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø–æ–ª–Ω–µ–Ω—ã")
        
        # 4. –°–æ–∑–¥–∞–Ω–∏–µ SparkSession —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ Spark Session...")
        spark = SparkSession.builder \
            .appName("KafkaMLStreamingDebug") \
            .config("spark.sql.adaptive.enabled", "true") \
            .getOrCreate()
        
        logger.info(f"‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω–∞: {spark.version}")
        logger.info(f"üìä Spark Config: {spark.conf.getAll()}")
        
        # 5. –¢–µ—Å—Ç –ø—Ä–æ—Å—Ç–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏ Spark
        logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±–∞–∑–æ–≤—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π Spark...")
        test_data = [("test", 1), ("debug", 2)]
        test_df = spark.createDataFrame(test_data, ["name", "value"])
        count = test_df.count()
        logger.info(f"‚úÖ Spark —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: {count} –∑–∞–ø–∏—Å–µ–π –≤ —Ç–µ—Å—Ç–æ–≤–æ–º DataFrame")
        
        # 6. –°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã –±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ Kafka (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
        logger.info("üé≠ –°–∏–º—É–ª—è—Ü–∏—è Kafka ML Streaming (–±–µ–∑ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è)...")
        
        start_time = time.time()
        processed_batches = 0
        
        while time.time() - start_time < args.demo_duration:
            processed_batches += 1
            current_time = datetime.now().strftime("%H:%M:%S")
            
            # –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ batch'–∞
            logger.info(f"[{current_time}] üì¶ Batch #{processed_batches}: –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö...")
            
            # –ò–º–∏—Ç–∞—Ü–∏—è ML inference
            fake_predictions = processed_batches * 10
            logger.info(f"  ü§ñ –°–∏–º—É–ª—è—Ü–∏—è ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {fake_predictions} —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞–∂–¥—ã–µ 3 batch'–∞
            if processed_batches % 3 == 0:
                elapsed = int(time.time() - start_time)
                logger.info(f"  üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {processed_batches} batches –∑–∞ {elapsed}—Å")
            
            time.sleep(5)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É batch'–∞–º–∏
        
        # 7. –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_elapsed = int(time.time() - start_time)
        logger.info("üéØ DEBUG —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        logger.info(f"  ‚è±Ô∏è –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {total_elapsed} —Å–µ–∫—É–Ω–¥")
        logger.info(f"  üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ batches: {processed_batches}")
        logger.info(f"  üîó Kafka (—Å–∏–º—É–ª—è—Ü–∏—è): {args.kafka_brokers}")
        logger.info(f"  üèÜ –ú–æ–¥–µ–ª—å (—Å–∏–º—É–ª—è—Ü–∏—è): {args.model_name} ({args.model_stage})")
        
        spark.stop()
        logger.info("‚úÖ Kafka ML Streaming DEBUG –∑–∞–≤–µ—Ä—à–∏–ª—Å—è —É—Å–ø–µ—à–Ω–æ!")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ DEBUG –≤–µ—Ä—Å–∏–∏: {e}")
        import traceback
        logger.error(f"üìú –ü–æ–ª–Ω—ã–π traceback:\n{traceback.format_exc()}")
        
        try:
            if 'spark' in locals():
                spark.stop()
                logger.info("üõë Spark Session –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞")
        except Exception as stop_error:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ Spark: {stop_error}")
            
        sys.exit(1)

if __name__ == "__main__":
    main()
