#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–æ–≤—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —á—Ç–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏–∑ S3 –±–µ–∑ Kafka
"""
import argparse
import logging
import sys
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    parser = argparse.ArgumentParser(description='Test S3 Data Reader')
    parser.add_argument('--s3-bucket', required=True)
    parser.add_argument('--file-path', default='train/data.csv')
    
    args = parser.parse_args()
    
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ S3 Data Reader Test")
        
        # –°–æ–∑–¥–∞–µ–º SparkSession
        spark = SparkSession.builder \
            .appName("S3DataReaderTest") \
            .getOrCreate()
        
        logger.info(f"‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω–∞: {spark.version}")
        logger.info(f"üì¶ S3 Bucket: {args.s3_bucket}")
        logger.info(f"üìÅ File path: {args.file_path}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ 
        logger.info("üìä –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        test_data = []
        for i in range(100):
            transaction = {
                'transaction_id': f'tx_{i:06d}',
                'user_id': f'user_{i % 20}',
                'merchant_id': f'merchant_{i % 10}',
                'transaction_amount': round(10.0 + (i % 1000) * 0.5, 2),
                'transaction_date': '2024-01-01',
                'is_fraud': 1 if i % 17 == 0 else 0  # ~6% fraud rate
            }
            test_data.append(transaction)
        
        # –°–æ–∑–¥–∞–µ–º DataFrame –∏–∑ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        df = spark.createDataFrame(test_data)
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω DataFrame —Å {df.count()} –∑–∞–ø–∏—Å—è–º–∏")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        logger.info("üìä –ü—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö:")
        df.show(10, truncate=False)
        
        # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        logger.info("üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
        df.describe(['transaction_amount']).show()
        
        fraud_count = df.filter(col('is_fraud') == 1).count()
        total_count = df.count()
        fraud_rate = fraud_count / total_count * 100
        
        logger.info(f"üîç Fraud Detection Stats:")
        logger.info(f"   Total transactions: {total_count}")
        logger.info(f"   Fraud transactions: {fraud_count}")
        logger.info(f"   Fraud rate: {fraud_rate:.2f}%")
        
        # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
        user_stats = df.groupBy('user_id').agg(
            count('*').alias('transaction_count'),
            sum('transaction_amount').alias('total_amount'),
            sum('is_fraud').alias('fraud_count')
        ).orderBy(desc('transaction_count'))
        
        logger.info("üë• –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π:")
        user_stats.show(5)
        
        spark.stop()
        logger.info("‚úÖ S3 Data Reader Test –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        logger.error(f"üìú Traceback: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
