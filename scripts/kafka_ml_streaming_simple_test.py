#!/usr/bin/env python3
"""
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç Kafka ML Streaming
"""
import argparse
import sys
import time
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='[%(asctime)s] %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main():
    logger.info("üöÄ –ü–†–û–°–¢–û–ô –¢–ï–°–¢: –ó–∞–ø—É—Å–∫...")
    
    try:
        # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ - –ø—Ä–∏–Ω–∏–º–∞–µ–º –≤—Å–µ, –Ω–æ –Ω–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ
        parser = argparse.ArgumentParser(description='Simple Test')
        parser.add_argument('--kafka-brokers', default='test:9092')
        parser.add_argument('--input-topic', default='test')
        parser.add_argument('--output-topic', default='test')
        parser.add_argument('--metrics-topic', default='test')
        parser.add_argument('--mlflow-uri', default='http://test:5000')
        parser.add_argument('--model-name', default='test_model')
        parser.add_argument('--model-stage', default='test')
        parser.add_argument('--fraud-threshold', type=float, default=0.5)
        parser.add_argument('--batch-timeout', default='30 seconds')
        parser.add_argument('--max-offsets', type=int, default=1000)
        parser.add_argument('--demo-duration', type=int, default=30)
        parser.add_argument('--checkpoint-location', default='test')
        parser.add_argument('--app-name', default='test')
        
        args = parser.parse_args()
        logger.info("‚úÖ –ü–†–û–°–¢–û–ô –¢–ï–°–¢: –ê—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã")
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        logger.info(f"üîó Kafka Brokers: {args.kafka_brokers}")
        logger.info(f"üì• Input Topic: {args.input_topic}")
        logger.info(f"ü§ñ MLflow URI: {args.mlflow_uri}")
        logger.info(f"üèÜ Model: {args.model_name}")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ Python –æ–∫—Ä—É–∂–µ–Ω–∏—è
        logger.info(f"üêç Python version: {sys.version}")
        logger.info(f"üìÅ Python path: {sys.path[0]}")
        
        # –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ PySpark
        logger.info("üîß –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ PySpark...")
        try:
            from pyspark.sql import SparkSession
            logger.info("‚úÖ PySpark –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            
            # –°–æ–∑–¥–∞–Ω–∏–µ SparkSession
            logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ Spark Session...")
            spark = SparkSession.builder \
                .appName("SimpleKafkaTest") \
                .config("spark.sql.adaptive.enabled", "true") \
                .getOrCreate()
            
            logger.info(f"‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω–∞: {spark.version}")
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç Spark
            test_data = [("test", 1), ("simple", 2)]
            df = spark.createDataFrame(test_data, ["name", "value"])
            count = df.count()
            logger.info(f"‚úÖ Spark —Ç–µ—Å—Ç: {count} –∑–∞–ø–∏—Å–µ–π")
            
            # –°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã
            logger.info("üé≠ –°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã streaming job...")
            for i in range(3):
                logger.info(f"  üì¶ Batch {i+1}: –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
                time.sleep(5)
            
            spark.stop()
            logger.info("‚úÖ –ü–†–û–°–¢–û–ô –¢–ï–°–¢ –£–°–ü–ï–®–ï–ù!")
            sys.exit(0)
            
        except ImportError as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ PySpark: {e}")
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        sys.exit(1)

if __name__ == "__main__":
    main()
