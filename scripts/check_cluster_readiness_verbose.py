#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
‚úÖ –£–ª—É—á—à–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è Kafka Streaming
===================================================================

PySpark job –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º.

Author: ML Pipeline Team
Date: 2024
"""

from pyspark.sql import SparkSession
import argparse
import sys
import logging
import traceback

# Setup logging with more verbose output
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def check_spark_environment(spark):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ Spark environment —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    try:
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –ø—Ä–æ–≤–µ—Ä–∫—É Spark environment...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 1: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ DataFrame
        logger.info("üìù –®–∞–≥ 1: –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ DataFrame...")
        test_data = [(1, "test"), (2, "spark"), (3, "ready")]
        df = spark.createDataFrame(test_data, ["id", "value"])
        logger.info("‚úÖ DataFrame —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 2: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ count
        logger.info("üî¢ –®–∞–≥ 2: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ count...")
        count = df.count()
        logger.info(f"‚úÖ Count –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ: {count} –∑–∞–ø–∏—Å–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 3: –ü—Ä–æ—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        logger.info("üîÑ –®–∞–≥ 3: –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏...")
        filtered_df = df.filter(df.id > 1)
        filtered_count = filtered_df.count()
        logger.info(f"‚úÖ –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ: {filtered_count} –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 4: Python –º–æ–¥—É–ª–∏
        logger.info("üì¶ –®–∞–≥ 4: –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö Python –º–æ–¥—É–ª–µ–π...")
        
        basic_modules = ['json', 'datetime', 'os', 'sys']
        available_basic = 0
        for module_name in basic_modules:
            try:
                __import__(module_name)
                available_basic += 1
                logger.info(f"‚úÖ –ë–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å {module_name} –¥–æ—Å—Ç—É–ø–µ–Ω")
            except ImportError as e:
                logger.warning(f"‚ö†Ô∏è –ë–∞–∑–æ–≤—ã–π –º–æ–¥—É–ª—å {module_name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
        
        logger.info(f"üìä –ë–∞–∑–æ–≤—ã–µ –º–æ–¥—É–ª–∏: {available_basic}/{len(basic_modules)} –¥–æ—Å—Ç—É–ø–Ω—ã")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)
        optional_modules = ['pandas', 'numpy', 'requests']
        available_optional = 0
        for module_name in optional_modules:
            try:
                __import__(module_name)
                available_optional += 1
                logger.info(f"‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å {module_name} –¥–æ—Å—Ç—É–ø–µ–Ω")
            except ImportError:
                logger.info(f"‚ÑπÔ∏è –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –º–æ–¥—É–ª—å {module_name} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω (–Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ)")
        
        logger.info(f"üìä –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏: {available_optional}/{len(optional_modules)} –¥–æ—Å—Ç—É–ø–Ω—ã")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ 5: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Spark
        logger.info("‚öôÔ∏è –®–∞–≥ 5: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Spark...")
        try:
            spark_version = spark.version
            logger.info(f"‚úÖ Spark –≤–µ—Ä—Å–∏—è: {spark_version}")
            
            executor_instances = spark.conf.get('spark.executor.instances', '–Ω–µ –∑–∞–¥–∞–Ω–æ')
            logger.info(f"‚úÖ Executor instances: {executor_instances}")
            
            app_name = spark.conf.get('spark.app.name', '–Ω–µ –∑–∞–¥–∞–Ω–æ')
            logger.info(f"‚úÖ App name: {app_name}")
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ü—Ä–æ–±–ª–µ–º–∞ —Å –ø–æ–ª—É—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Spark: {e}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
        if available_basic >= len(basic_modules) * 0.8:  # 80% –±–∞–∑–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã
            logger.info("üéâ –í—Å–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
            return True
        else:
            logger.error("‚ùå –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –±–∞–∑–æ–≤—ã—Ö –º–æ–¥—É–ª–µ–π")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ check_spark_environment: {e}")
        logger.error(f"üîç Traceback: {traceback.format_exc()}")
        return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ DataProc –∫–ª–∞—Å—Ç–µ—Ä–∞...")
        
        # –ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤
        parser = argparse.ArgumentParser(
            description='–ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–ª—è Kafka Streaming'
        )
        parser.add_argument('--packages', help='–°–ø–∏—Å–æ–∫ –ø–∞–∫–µ—Ç–æ–≤ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)')
        parser.add_argument('--s3-endpoint-url', help='S3 endpoint URL (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)')
        parser.add_argument('--s3-access-key', help='S3 access key (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)')
        parser.add_argument('--s3-secret-key', help='S3 secret key (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ—Ç—Å—è)')
        
        args = parser.parse_args()
        logger.info(f"üìã –ê—Ä–≥—É–º–µ–Ω—Ç—ã –ø–æ–ª—É—á–µ–Ω—ã: packages={args.packages}")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ Spark Session
        logger.info("üîß –°–æ–∑–¥–∞–Ω–∏–µ Spark Session...")
        spark = SparkSession.builder \
            .appName("KafkaStreamingClusterCheck") \
            .getOrCreate()
        
        logger.info(f"‚úÖ Spark Session —Å–æ–∑–¥–∞–Ω–∞ —É—Å–ø–µ—à–Ω–æ: –≤–µ—Ä—Å–∏—è {spark.version}")
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏
        logger.info("üîç –ù–∞—á–∏–Ω–∞—é –∫–æ–º–ø–ª–µ–∫—Å–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É –∫–ª–∞—Å—Ç–µ—Ä–∞...")
        
        check_result = check_spark_environment(spark)
        
        if check_result:
            logger.info("üéâ –£–°–ü–ï–•: –ö–ª–∞—Å—Ç–µ—Ä –≥–æ—Ç–æ–≤ –¥–ª—è Kafka Streaming!")
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            logger.info("üìä –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–ª–∞—Å—Ç–µ—Ä–µ:")
            logger.info(f"  üîß Spark –≤–µ—Ä—Å–∏—è: {spark.version}")
            logger.info(f"  üêç Python –≤–µ—Ä—Å–∏—è: {sys.version.split()[0]}")
            logger.info(f"  üíª Executor instances: {spark.conf.get('spark.executor.instances', '–Ω–µ –∑–∞–¥–∞–Ω–æ')}")
            
            logger.info("‚úÖ –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É —Å –∫–æ–¥–æ–º 0 (—É—Å–ø–µ—Ö)")
            spark.stop()
            sys.exit(0)
        else:
            logger.error("‚ùå –û–®–ò–ë–ö–ê: –ö–ª–∞—Å—Ç–µ—Ä –ù–ï –≥–æ—Ç–æ–≤ –¥–ª—è Kafka Streaming")
            logger.error("üí° –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏")
            logger.info("‚ùå –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É —Å –∫–æ–¥–æ–º 1 (–æ—à–∏–±–∫–∞)")
            spark.stop()
            sys.exit(1)
            
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê –≤ main(): {e}")
        logger.error(f"üîç –ü–æ–ª–Ω—ã–π traceback: {traceback.format_exc()}")
        logger.info("‚ùå –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É —Å –∫–æ–¥–æ–º 1 (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞)")
        sys.exit(1)

if __name__ == "__main__":
    main()
