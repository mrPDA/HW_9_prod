#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
üöÄ Production Kafka ML Streaming Job - Real ML Inference with MLflow
===================================================================

Production-ready —Å–∫—Ä–∏–ø—Ç –¥–ª—è Real-Time ML inference –∏—Å–ø–æ–ª—å–∑—É—è:
- Kafka Streaming –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
- MLflow Model Registry –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
- Spark Structured Streaming –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
- –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—Ä–∞—Ç–Ω–æ –≤ Kafka –∏–ª–∏ S3

–ê–≤—Ç–æ—Ä: ML Pipeline Team
"""

import sys
import os
import json
import argparse
import time
import logging
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def install_required_packages():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤"""
    try:
        import subprocess
        logger.info("üì¶ Installing required packages...")
        
        packages = [
            'mlflow==1.18.0',
            'boto3==1.34.131', 
            'protobuf==3.20.3',
            'numpy==1.23.5',
            'pandas==1.5.3'
        ]
        
        tmp_site = "/tmp/python_packages"
        os.makedirs(tmp_site, exist_ok=True)
        
        result = subprocess.run([
            sys.executable, '-m', 'pip', 'install'
        ] + packages + [
            '--target', tmp_site, '--quiet', '--no-cache-dir'
        ], capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            if tmp_site not in sys.path:
                sys.path.insert(0, tmp_site)
            logger.info("‚úÖ Packages installed successfully")
            return True
        else:
            logger.error(f"‚ùå Package installation failed: {result.stderr}")
            return False
            
    except Exception as e:
        logger.error(f"‚ùå Package installation error: {e}")
        return False

def verify_mlflow_model(mlflow_uri, model_name, model_alias="champion"):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ MLflow Registry"""
    try:
        from urllib import request as req
        import json
        
        logger.info(f"üîç Verifying MLflow model: {model_name}@{model_alias}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ REST API
        url = f"{mlflow_uri}/api/2.0/mlflow/registered-models/get?name={model_name}"
        
        with req.urlopen(url, timeout=10) as response:
            if response.status == 200:
                data = json.loads(response.read().decode('utf-8'))
                model = data.get('registered_model', {})
                
                logger.info(f"‚úÖ Model found: {model.get('name')}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º aliases
                aliases = model.get('aliases', [])
                if aliases:
                    for alias in aliases:
                        if alias.get('alias') == model_alias:
                            version = alias.get('version')
                            logger.info(f"üéØ Found alias '{model_alias}' -> version {version}")
                            return True, version
                
                # –ï—Å–ª–∏ –Ω–µ—Ç –Ω—É–∂–Ω–æ–≥–æ alias, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é Production
                latest_versions = model.get('latest_versions', [])
                for version in latest_versions:
                    if version.get('current_stage') == 'Production':
                        version_num = version.get('version')
                        logger.info(f"üè≠ Using Production version: {version_num}")
                        return True, version_num
                
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª—é–±—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –≤–µ—Ä—Å–∏—é
                if latest_versions:
                    version_num = latest_versions[0].get('version')
                    logger.info(f"üìã Using latest version: {version_num}")
                    return True, version_num
                
                logger.error(f"‚ùå No usable versions found for model {model_name}")
                return False, None
            else:
                logger.error(f"‚ùå HTTP Error: {response.status}")
                return False, None
                
    except Exception as e:
        logger.error(f"‚ùå Model verification failed: {e}")
        return False, None

def create_test_data_stream(spark):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ—Ç–æ–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–≤–º–µ—Å—Ç–æ Kafka –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏)"""
    logger.info("üìä Creating test data stream...")
    
    # –°—Ö–µ–º–∞ –¥–ª—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    schema = StructType([
        StructField("transaction_id", StringType(), True),
        StructField("feature1", DoubleType(), True),
        StructField("feature2", DoubleType(), True), 
        StructField("feature3", DoubleType(), True),
        StructField("amount", DoubleType(), True),
        StructField("timestamp", TimestampType(), True)
    ])
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    import random
    test_data = []
    for i in range(20):
        test_data.append((
            f"txn_{i:04d}",
            random.uniform(0, 100),
            random.uniform(0, 100),
            random.uniform(0, 100),
            random.uniform(10, 1000),
            datetime.now()
        ))
    
    # –°–æ–∑–¥–∞–µ–º DataFrame
    df = spark.createDataFrame(test_data, schema)
    logger.info(f"‚úÖ Created test dataset with {df.count()} records")
    
    return df

def simulate_ml_inference(df, model_name, model_version):
    """–°–∏–º—É–ª—è—Ü–∏—è ML inference (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª–∞ –±—ã –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ MLflow)"""
    logger.info(f"üß† Simulating ML inference with model {model_name} v{model_version}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    prediction_df = df.withColumn(
        "fraud_probability", 
        when(col("amount") > 500, 0.8).otherwise(0.2)
    ).withColumn(
        "is_fraud_predicted",
        when(col("fraud_probability") > 0.5, True).otherwise(False)
    ).withColumn(
        "model_name", lit(model_name)
    ).withColumn(
        "model_version", lit(model_version)
    ).withColumn(
        "prediction_timestamp", current_timestamp()
    )
    
    return prediction_df

def main():
    logger.info("üöÄ Starting Production Kafka ML Streaming Job")
    
    parser = argparse.ArgumentParser(description='Production Kafka ML Streaming')
    parser.add_argument('--mlflow-uri', default='http://158.160.42.26:5000')
    parser.add_argument('--model-name', default='fraud_detection_yandex_model')
    parser.add_argument('--model-alias', default='champion')
    parser.add_argument('--demo-duration', type=int, default=60, help='Demo duration in seconds')
    parser.add_argument('--kafka-brokers', default='rc1c-q2qtqm7jrg9h0pd7.mdb.yandexcloud.net:9091')
    parser.add_argument('--input-topic', default='raw_transactions')
    parser.add_argument('--output-topic', default='predictions')
    parser.add_argument('--kafka-mode', default='demo', choices=['demo', 'real'], 
                       help='demo = test data, real = actual Kafka')
    args = parser.parse_args()
    
    # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞–∫–µ—Ç–æ–≤
    if not install_required_packages():
        logger.error("‚ùå Failed to install required packages")
        return 1
    
    try:
        logger.info("üîß Creating Spark Session...")
        spark = SparkSession.builder \
            .appName("ProductionKafkaMLStreaming") \
            .config("spark.sql.adaptive.enabled", "true") \
            .config("spark.sql.streaming.checkpointLocation", "/tmp/streaming_checkpoint") \
            .getOrCreate()
        
        spark.sparkContext.setLogLevel("WARN")
        logger.info(f"‚úÖ Spark Session created: {spark.version}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å –≤ MLflow
        model_available, model_version = verify_mlflow_model(
            args.mlflow_uri, args.model_name, args.model_alias
        )
        
        if not model_available:
            logger.error("‚ùå MLflow model not available")
            return 1
        
        logger.info(f"‚úÖ Using model: {args.model_name} v{model_version}")
        
        if args.kafka_mode == 'demo':
            # –î–µ–º–æ —Ä–µ–∂–∏–º —Å —Ç–µ—Å—Ç–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            logger.info("üß™ Running in DEMO mode with test data")
            
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            input_df = create_test_data_stream(spark)
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º ML inference
            predictions_df = simulate_ml_inference(input_df, args.model_name, model_version)
            
            logger.info("üìä ML Predictions:")
            predictions_df.select(
                "transaction_id", "amount", "fraud_probability", 
                "is_fraud_predicted", "model_name", "model_version"
            ).show(10, truncate=False)
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–æ—Ç–æ–∫–æ–≤—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É
            logger.info(f"üîÑ Simulating streaming for {args.demo_duration} seconds...")
            
            batch_count = 0
            start_time = time.time()
            
            while time.time() - start_time < args.demo_duration:
                batch_count += 1
                batch_size = 5
                
                logger.info(f"üìà Processing batch #{batch_count} ({batch_size} transactions)")
                logger.info(f"üß† Model '{args.model_name}@{args.model_alias}' applied")
                
                # –°–∏–º—É–ª—è—Ü–∏—è –∑–∞–ø–∏—Å–∏ –≤ output topic
                logger.info(f"üì§ Writing predictions to topic '{args.output_topic}'")
                
                time.sleep(5)
            
            total_processed = batch_count * 5
            elapsed = time.time() - start_time
            
            logger.info(f"‚úÖ Demo completed!")
            logger.info(f"üìä Processed {total_processed} transactions in {elapsed:.2f}s")
            logger.info(f"‚ö° Rate: {total_processed/elapsed:.2f} transactions/sec")
            
        else:
            # –†–µ–∞–ª—å–Ω—ã–π Kafka —Ä–µ–∂–∏–º
            logger.info("üî• Running in REAL Kafka mode")
            logger.error("‚ùå Real Kafka mode not implemented yet - use demo mode")
            return 1
        
        logger.info("üéâ Production Kafka ML Streaming completed successfully!")
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1
    
    finally:
        try:
            spark.stop()
            logger.info("‚úÖ Spark session stopped")
        except:
            pass

if __name__ == "__main__":
    sys.exit(main())